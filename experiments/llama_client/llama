#!/usr/bin/env ruby

require "llama_cpp"
require "bundler/setup"
params = LLaMACpp::ContextParams.new
params.seed = 12

# To download the model, run from the root of the repo:
# mkdir ./experiments/llama_client/models
# git lfs install
# git clone git@hf.co:chharlesonfire/ggml-vicuna-7b-4bit ./experiments/llama_client/models/
model_path = "./experiments/llama_client/models/ggml-vicuna-7b-4bit/ggml-vicuna-7b-q4_0.bin"

context = LLaMACpp::Context.new(model_path: model_path, params: params)

puts LLaMACpp.generate(context, "How do concatenate arrays in Ruby?", n_threads: 4)
